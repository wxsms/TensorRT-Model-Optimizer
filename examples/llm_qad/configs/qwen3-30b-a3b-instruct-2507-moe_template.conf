#!/bin/bash
########################################################
# QAD Configuration: Qwen3-30B-A3B Instruct (MoE)
# Mixture of Experts - requires more resources
#
# Usage:
#   sbatch sbatch_qad.sh --config configs/qwen3-30b-a3b-instruct-2507-moe_template.conf
########################################################

########################################################
# MODEL
########################################################
export STUDENT_MODEL="Qwen3-30B-A3B-Instruct-2507"
export TEACHER_MODEL="Qwen3-30B-A3B-Instruct-2507"
export TOKENIZER_MODEL="Qwen/Qwen3-30B-A3B-Instruct-2507"

########################################################
# CHECKPOINTS (REQUIRED)
########################################################
export STUDENT_CKPT="" # Student MLM checkpoint path
export TEACHER_CKPT="" # Teacher MLM checkpoint path
export TEACHER_MODEL_CONFIG="" # Teacher MLM model config yaml file, e.g., configs/Qwen3-30B-A3B-teacher.yaml

########################################################
# TRAINING (REQUIRED - no defaults in qwen_qad.sh)
########################################################
export LR="5e-6"
export GBS=64
export MIN_LR="1e-8"
export LR_DECAY_STYLE="cosine"
export SAVE_INTERVAL=200
export LOG_INTERVAL=10
export DATASET_NAME="openscience_nemotron"  # use for logging
export TRAIN_SAMPLES=5120000

########################################################
# PARALLELISM
# Note: QAD loads both student + teacher models, requires more memory
########################################################
export TP_SIZE=2
export PP_SIZE=1
export MBS=2
export NUM_GPUS=4
export MASTER_PORT=29500

########################################################
# MOE
########################################################
export EP_SIZE=4
export IS_MOE=false

########################################################
# PATHS (REQUIRED - no defaults in qwen_qad.sh)
########################################################
export MLM_DIR="" # path to Megatron-LM source directory
export MODELOPT_DIR="" # path to Model-Optimizer source directory
export STUDENT_CONFIG_FILE="" # path to student model args script, e.g., ${MLM_DIR}/examples/post_training/modelopt/conf/Qwen/Qwen3-30B-A3B.sh
export QAD_CHECKPOINT_ROOT="" # path to store QAD checkpoints
export DATACACHE_DIR="" # path to data cache directory

########################################################
# CONTAINER
########################################################
export CONTAINER_IMAGE="" # path to container image, e.g., nvcr.io/nvidia/pytorch:25.06-py3
export CONTAINER_MOUNTS="" # container mounts, e.g., "/lustre/fs1:/lustre/fs1"
export CONTAINER_WORKDIR="" # container work directory, e.g., "<path-to-modelopt>/Model-Optimizer/examples/llm_qad"


########################################################
# DATASET
########################################################
# Generate with: bash data_utils/generate_dataset.sh --output-dir <path> --mlm-path <path> --tokenizer <model>
export BLEND_PATH="" # path to datablend_combined.json from generate_dataset.sh