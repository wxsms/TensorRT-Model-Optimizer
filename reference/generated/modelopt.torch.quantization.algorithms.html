

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>algorithms &mdash; Model Optimizer 0.19.0</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=7a224f4b" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=20d3d275"></script>
      <script src="../../_static/tabs.js?v=3ee01567"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="calib" href="modelopt.torch.quantization.calib.html" />
    <link rel="prev" title="quantization" href="modelopt.torch.quantization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            TensorRT Model Optimizer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/1_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/2_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/3_quantization.html">Quick Start: Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/4_pruning.html">Quick Start: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/5_distillation.html">Quick Start: Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/6_sparsity.html">Quick Start: Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/1_quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/2_pruning.html">Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/3_nas.html">NAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/4_distillation.html">Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/5_sparsity.html">Sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/6_save_load.html">Saving &amp; Restoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/7_speculative_decoding.html">Speculative Decoding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/1_tensorrt_llm_deployment.html">TensorRT-LLM Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples/0_all_examples.html">All GitHub Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/1_cifar_resnet.html">ResNet20 on CIFAR-10: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/2_bert_prune_distill_quantize.html">HF BERT: Prune, Distill &amp; Quantize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_changelog.html">Model Optimizer Changelog</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../1_modelopt_api.html">modelopt API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modelopt.deploy.html">deploy</a></li>
<li class="toctree-l2"><a class="reference internal" href="modelopt.onnx.html">onnx</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="modelopt.torch.html">torch</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.distill.html">distill</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.export.html">export</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.nas.html">nas</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.opt.html">opt</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.prune.html">prune</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="modelopt.torch.quantization.html">quantization</a><ul class="current">
<li class="toctree-l4 current"><a class="current reference internal" href="#">algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.calib.html">calib</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.config.html">config</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.conversion.html">conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.export_onnx.html">export_onnx</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.extensions.html">extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.mode.html">mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.model_calib.html">model_calib</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.model_quant.html">model_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.nn.html">nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.optim.html">optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.plugins.html">plugins</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.qtensor.html">qtensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.quant_modules.html">quant_modules</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.tensor_quant.html">tensor_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.utils.html">utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.sparsity.html">sparsity</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.speculative.html">speculative</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.trace.html">trace</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.utils.html">utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../support/1_contact.html">Contact us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support/2_faqs.html">FAQs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TensorRT Model Optimizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../1_modelopt_api.html">modelopt API</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.html">torch</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.quantization.html">quantization</a></li>
      <li class="breadcrumb-item active">algorithms</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/reference/generated/modelopt.torch.quantization.algorithms.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="algorithms">
<h1>algorithms<a class="headerlink" href="#algorithms" title="Link to this heading"></a></h1>
<p id="module-modelopt.torch.quantization.algorithms">Module for advanced quantization algorithms.</p>
<p class="rubric">Classes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher" title="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher"><code class="xref py py-obj docutils literal notranslate"><span class="pre">AutoQuantizeSearcher</span></code></a></p></td>
<td><p>A searcher for AutoQuantize algorithm.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.algorithms.QuantRecipe" title="modelopt.torch.quantization.algorithms.QuantRecipe"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantRecipe</span></code></a></p></td>
<td><p>A subclass of QuantizeConfig enabling auto_quantize specific configurations.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.algorithms.QuantRecipeHparam" title="modelopt.torch.quantization.algorithms.QuantRecipeHparam"><code class="xref py py-obj docutils literal notranslate"><span class="pre">QuantRecipeHparam</span></code></a></p></td>
<td><p>An Hparam for quantization recipes.</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AutoQuantizeSearcher</span></span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="modelopt.torch.opt.searcher.html#modelopt.torch.opt.searcher.BaseSearcher" title="modelopt.torch.opt.searcher.BaseSearcher"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaseSearcher</span></code></a></p>
<p>A searcher for AutoQuantize algorithm.</p>
<p>In AutoQuantize, we search for the best per-layer quantization configuration that minimizes the sum of per-layer
scores while meeting the specified constraint. AutoQuantize uses Linear Programming Solver to find the
optimal quantization configuration.</p>
<p>The auto_quantize score for a layer quantization configuration is an approximation of model loss change change due
to quantizing the particular layer with the particular configuration.
The approximation is based on taylor expansion of the loss function wrt to the quantized output of the layer and
substitution of Fisher information for Hessian.
This approximation is mathematically correct for models where the loss
is a log likelihood loss such as BERT, GPT, etc. However, the auto_quantize score can still be used as a proxy
for other models such as ResNet.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.before_search">
<span class="sig-name descname"><span class="pre">before_search</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.before_search" title="Link to this definition"></a></dt>
<dd><p>Prepare the model for search by calibrating the quantizers  and collecting <code class="docutils literal notranslate"><span class="pre">AutoQuantize</span></code> score.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.best">
<span class="sig-name descname"><span class="pre">best</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.best" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.candidate_stats">
<span class="sig-name descname"><span class="pre">candidate_stats</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.candidate_stats" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.default_search_config">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_search_config</span></span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.default_search_config" title="Link to this definition"></a></dt>
<dd><p>Get the default config for the searcher.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.default_state_dict">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">default_state_dict</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Any</span><span class="p"><span class="pre">]</span></span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.default_state_dict" title="Link to this definition"></a></dt>
<dd><p>Get the default state dict for AutoQuantize.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.gradient_checkpointing_enable_contexts">
<span class="sig-name descname"><span class="pre">gradient_checkpointing_enable_contexts</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">List</span><span class="p"><span class="pre">[</span></span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">Callable</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">[(&lt;function</span> <span class="pre">_is_supported_hf_model&gt;,</span> <span class="pre">&lt;function</span> <span class="pre">setup_model_for_gradient_checkpointing&gt;)]</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.gradient_checkpointing_enable_contexts" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.insert_hparams_after_merge_rules">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">insert_hparams_after_merge_rules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quant_recipes</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.insert_hparams_after_merge_rules" title="Link to this definition"></a></dt>
<dd><p>Restrict the search space using the merge rules and insert the hparams for the model.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.register_gradient_checkpointing_enable_context">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">register_gradient_checkpointing_enable_context</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">is_supported_checker</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.register_gradient_checkpointing_enable_context" title="Link to this definition"></a></dt>
<dd><p>Register a gradient checkpointing enable context for <cite>AutoQuantize</cite> score estimation.</p>
<p>If the <cite>is_supported_checker(model)</cite> returns True, the <cite>context(model)</cite> will be used to enable gradient
checkpointing.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>is_supported_checker</strong> (<em>Callable</em>) – </p></li>
<li><p><strong>context</strong> (<em>Callable</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.rules">
<span class="sig-name descname"><span class="pre">rules</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">['^(.*?)\\.(q_proj|k_proj|v_proj)$',</span> <span class="pre">'^(.*?)\\.(gate_proj|up_proj)$',</span> <span class="pre">'^(.*?)\\.(\\d+\\.(w1|w2|w3))$',</span> <span class="pre">'^(.*?)\\.((w1_linear|w2_linear|w3_linear)\\.\\d+)$']</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.rules" title="Link to this definition"></a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.run_search">
<span class="sig-name descname"><span class="pre">run_search</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.run_search" title="Link to this definition"></a></dt>
<dd><p>Search for the best per-layer quantization configuration and return the best model and configuration.</p>
<p>AutoQuantize uses Linear Programming Solver to find the optimal quantization configuration which
minimizes the sum of per-layer auto_quantize scores while meeting the specified constraint.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.sanitize_search_config">
<span class="sig-name descname"><span class="pre">sanitize_search_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.sanitize_search_config" title="Link to this definition"></a></dt>
<dd><p>Sanitize the search config dict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>config</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>None</em>) – </p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Dict</em>[<em>str</em>, <em>Any</em>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">QuantRecipe</span></span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">CustomHPType</span></code></p>
<p>A subclass of QuantizeConfig enabling auto_quantize specific configurations.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.__init__" title="Link to this definition"></a></dt>
<dd><p>Initialize the QuantRecipe with the name of the quantization format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>name</strong> (<em>str</em><em> | </em><em>None</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.compression">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">compression</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">float</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.compression" title="Link to this definition"></a></dt>
<dd><p>Get the compression factor for the quantization format.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.config">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">config</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference internal" href="modelopt.torch.quantization.config.html#modelopt.torch.quantization.config.QuantizeConfig" title="modelopt.torch.quantization.config.QuantizeConfig"><span class="pre">QuantizeConfig</span></a></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.config" title="Link to this definition"></a></dt>
<dd><p>Get the quantization configuration for the quantization format.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.disable_folding_pqs_to_weights">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">disable_folding_pqs_to_weights</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.disable_folding_pqs_to_weights" title="Link to this definition"></a></dt>
<dd><p>Disable the folding of pre_quant_scale to weights.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.fold_pqs_to_weights">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">fold_pqs_to_weights</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.fold_pqs_to_weights" title="Link to this definition"></a></dt>
<dd><p>Fold the pre_quant_scale in weight_quantizers to weights.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipe.num_bits">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">num_bits</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">int</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipe.num_bits" title="Link to this definition"></a></dt>
<dd><p>Get the number of bits for the quantization format.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipeHparam">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">QuantRecipeHparam</span></span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipeHparam" title="Link to this definition"></a></dt>
<dd><p>Bases: <a class="reference internal" href="modelopt.torch.opt.hparam.html#modelopt.torch.opt.hparam.Hparam" title="modelopt.torch.opt.hparam.Hparam"><code class="xref py py-class docutils literal notranslate"><span class="pre">Hparam</span></code></a></p>
<p>An Hparam for quantization recipes.</p>
<p>In addition, this Hparam also:
1. Keeps a link to its modules and sets the quantizers for the module based on the active recipe.
2. Keeps track of the importance of each recipe in a dict instead of a tensor</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipeHparam.__init__">
<span class="sig-name descname"><span class="pre">__init__</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">choices</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">original</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nn_modules</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipeHparam.__init__" title="Link to this definition"></a></dt>
<dd><p>Initializes Hparam with original value and choices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>choices</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="#modelopt.torch.quantization.algorithms.QuantRecipe" title="modelopt.torch.quantization.algorithms.QuantRecipe"><em>QuantRecipe</em></a><em>]</em>) – </p></li>
<li><p><strong>original</strong> (<a class="reference internal" href="#modelopt.torch.quantization.algorithms.QuantRecipe" title="modelopt.torch.quantization.algorithms.QuantRecipe"><em>QuantRecipe</em></a><em> | </em><em>None</em>) – </p></li>
<li><p><strong>nn_modules</strong> (<em>List</em><em>[</em><em>Module</em><em>] </em><em>| </em><em>None</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>None</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipeHparam.active">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">active</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">CustomHPType</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipeHparam.active" title="Link to this definition"></a></dt>
<dd><p>Return the currently active value.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="modelopt.torch.quantization.algorithms.QuantRecipeHparam.importance">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">importance</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="pre">Dict</span></em><a class="headerlink" href="#modelopt.torch.quantization.algorithms.QuantRecipeHparam.importance" title="Link to this definition"></a></dt>
<dd><p>Return the importance dict mapping recipe and importance.</p>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modelopt.torch.quantization.html" class="btn btn-neutral float-left" title="quantization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modelopt.torch.quantization.calib.html" class="btn btn-neutral float-right" title="calib" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>