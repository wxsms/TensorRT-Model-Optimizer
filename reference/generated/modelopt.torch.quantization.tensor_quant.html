

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>tensor_quant &mdash; Model Optimizer 0.19.0</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=7a224f4b" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=20d3d275"></script>
      <script src="../../_static/tabs.js?v=3ee01567"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="utils" href="modelopt.torch.quantization.utils.html" />
    <link rel="prev" title="quant_modules" href="modelopt.torch.quantization.quant_modules.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            TensorRT Model Optimizer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/1_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/2_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/3_quantization.html">Quick Start: Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/4_pruning.html">Quick Start: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/5_distillation.html">Quick Start: Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/6_sparsity.html">Quick Start: Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/1_quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/2_pruning.html">Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/3_nas.html">NAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/4_distillation.html">Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/5_sparsity.html">Sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/6_save_load.html">Saving &amp; Restoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/7_speculative_decoding.html">Speculative Decoding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/1_tensorrt_llm_deployment.html">TensorRT-LLM Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples/0_all_examples.html">All GitHub Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/1_cifar_resnet.html">ResNet20 on CIFAR-10: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/2_bert_prune_distill_quantize.html">HF BERT: Prune, Distill &amp; Quantize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_changelog.html">Model Optimizer Changelog</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../1_modelopt_api.html">modelopt API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modelopt.deploy.html">deploy</a></li>
<li class="toctree-l2"><a class="reference internal" href="modelopt.onnx.html">onnx</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="modelopt.torch.html">torch</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.distill.html">distill</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.export.html">export</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.nas.html">nas</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.opt.html">opt</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.prune.html">prune</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="modelopt.torch.quantization.html">quantization</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.algorithms.html">algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.calib.html">calib</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.config.html">config</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.conversion.html">conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.export_onnx.html">export_onnx</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.extensions.html">extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.mode.html">mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.model_calib.html">model_calib</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.model_quant.html">model_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.nn.html">nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.optim.html">optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.plugins.html">plugins</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.qtensor.html">qtensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.quant_modules.html">quant_modules</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">tensor_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.utils.html">utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.sparsity.html">sparsity</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.speculative.html">speculative</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.trace.html">trace</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.utils.html">utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../support/1_contact.html">Contact us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support/2_faqs.html">FAQs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TensorRT Model Optimizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../1_modelopt_api.html">modelopt API</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.html">torch</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.quantization.html">quantization</a></li>
      <li class="breadcrumb-item active">tensor_quant</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/reference/generated/modelopt.torch.quantization.tensor_quant.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tensor-quant">
<h1>tensor_quant<a class="headerlink" href="#tensor-quant" title="Link to this heading"></a></h1>
<p id="module-modelopt.torch.quantization.tensor_quant">Basic tensor quantization functions.</p>
<p class="rubric">Classes</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction" title="modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DynamicBlockQuantizationFunction</span></code></a></p></td>
<td><p>Dynamic block quantization functional.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction" title="modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeAffineTensorQuantFunction</span></code></a></p></td>
<td><p>Fake version of affine quantization.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction" title="modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">FakeTensorQuantFunction</span></code></a></p></td>
<td><p>Fake version of TensorQuantFunction use CUDA extension.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction" title="modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LegacyFakeTensorQuantFunction</span></code></a></p></td>
<td><p>Fake version of TensorQuantFunction.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.ScaledE4M3Function" title="modelopt.torch.quantization.tensor_quant.ScaledE4M3Function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ScaledE4M3Function</span></code></a></p></td>
<td><p>E4M3fy input with scale.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.TensorQuantFunction" title="modelopt.torch.quantization.tensor_quant.TensorQuantFunction"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TensorQuantFunction</span></code></a></p></td>
<td><p>A universal tensor quantization function.</p></td>
</tr>
</tbody>
</table>
<p class="rubric">Functions</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.fake_quant_impl" title="modelopt.torch.quantization.tensor_quant.fake_quant_impl"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fake_quant_impl</span></code></a></p></td>
<td><p>Implementation of fake quantizing input according to number of bits.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.quantize_op_abstract" title="modelopt.torch.quantization.tensor_quant.quantize_op_abstract"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize_op_abstract</span></code></a></p></td>
<td><p>Register an abstract implementation for quantizing tensor.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.tensor_quant.scaled_e4m3_impl" title="modelopt.torch.quantization.tensor_quant.scaled_e4m3_impl"><code class="xref py py-obj docutils literal notranslate"><span class="pre">scaled_e4m3_impl</span></code></a></p></td>
<td><p>Implementation of fake quantizing input to FP8.</p></td>
</tr>
</tbody>
</table>
<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">DynamicBlockQuantizationFunction</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>Dynamic block quantization functional.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation with clipping.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">block_size</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale_bits</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.DynamicBlockQuantizationFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Forward method.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">FakeAffineTensorQuantFunction</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>Fake version of affine quantization.</p>
<p>gemmlowp style scale+shift quantization. See more details in
<a class="reference external" href="https://github.com/google/gemmlowp/blob/master/doc/quantization.md" rel="noopener noreferrer" target="_blank">https://github.com/google/gemmlowp/blob/master/doc/quantization.md</a>.</p>
<p>We DO NOT recommend affine quantization on weights for performance reason. There might be value to affine quantize
activation as it can be cancelled by bias and comes with no performance penalty. This functionality is only added
for experimental purpose.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation with clipping.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – Pytorch convention.</p></li>
<li><p><strong>grad_output</strong> – A tensor of gradient of outputs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor of gradient</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>grad_inputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">min_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_range</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeAffineTensorQuantFunction.forward" title="Link to this definition"></a></dt>
<dd><p>As it will be only applied on activation with per tensor granularity, broadcast is not needed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – Pytorch convention.</p></li>
<li><p><strong>inputs</strong> – A Tensor of type float32.</p></li>
<li><p><strong>min_range</strong> – A float.</p></li>
<li><p><strong>max_range</strong> – A float.</p></li>
<li><p><strong>num_bits</strong> – An integer</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Tensor of type output_dtype</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>outputs</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">FakeTensorQuantFunction</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>Fake version of TensorQuantFunction use CUDA extension.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation with clipping.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Forward method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.symbolic">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">symbolic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.FakeTensorQuantFunction.symbolic" title="Link to this definition"></a></dt>
<dd><p>ONNX symbolic function.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">LegacyFakeTensorQuantFunction</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>Fake version of TensorQuantFunction.</p>
<p>See comments of TensorQuantFunction, arguments are the same.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.LegacyFakeTensorQuantFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Forward method.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.ScaledE4M3Function">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">ScaledE4M3Function</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.ScaledE4M3Function" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>E4M3fy input with scale.</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation with clipping.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">E</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">M</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.forward" title="Link to this definition"></a></dt>
<dd><p>Forward method.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.symbolic">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">symbolic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">E</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">4</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">M</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.ScaledE4M3Function.symbolic" title="Link to this definition"></a></dt>
<dd><p>ONNX symbolic function.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.TensorQuantFunction">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">TensorQuantFunction</span></span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.TensorQuantFunction" title="Link to this definition"></a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></p>
<p>A universal tensor quantization function.</p>
<p>Take an input tensor, output an quantized tensor. The granularity of scale can be interpreted from the
shape of amax.
output_dtype indicates whether the quantized value will be stored in integer or float. The reason we want to store
it in float is the pytorch function takes the quantized value may not accept integer input, e.g. Conv2D.</p>
<p>It uses 2^num_bits -1 values instead of 2^num_bits. e.g., for num_bits=8, it uses [-127, 127] instead of [-128, 127]</p>
<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.TensorQuantFunction.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad_scale</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.TensorQuantFunction.backward" title="Link to this definition"></a></dt>
<dd><p>Implements straight through estimation with clipping.</p>
<p>For -amax &lt;= input &lt;= amax the gradient passes straight through, otherwise the gradient is zero.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – A Context object with saved tensors from forward.</p></li>
<li><p><strong>grad_outputs</strong> – A tensor of gradient of outputs.</p></li>
<li><p><strong>grad_scale</strong> – A tensor of gradient of scale.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor of gradient.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>grad_inputs</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.TensorQuantFunction.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.TensorQuantFunction.forward" title="Link to this definition"></a></dt>
<dd><p>Forward method.</p>
<p>Follow tensorflow convention, max value is passed in and used to decide scale, instead of inputing scale
directly. Though inputing scale directly may be more natural to use.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> – A Context object to store tensors for backward.</p></li>
<li><p><strong>inputs</strong> – A Tensor of type float32.</p></li>
<li><p><strong>amax</strong> – A Tensor of type float32. Inputs will be quantized within range [-amax, amax]
amax will be broadcasted to inputs tensor.</p></li>
<li><p><strong>num_bits</strong> – A integer used to calculate scaling factor, scale = (2^(num_bits-1) - 1) / max
Effectively, it indicates how many integer bits is used to represent the value. Default 8.</p></li>
<li><p><strong>output_dtype</strong> – A type of Tensor. torch.int32 or torch.float32.</p></li>
<li><p><strong>unsigned</strong> – A boolean. Use unsigned integer range. E.g. [0, 255] for num_bits=8. Default False.</p></li>
<li><p><strong>narrow_range</strong> – A boolean. Use symmetric integer range for signed quantization
E.g. [-127,127] instead of [-128,127] for num_bits=8. Default True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A Tensor of type output_dtype.
scale: A Tensor of type float32. outputs / scale will dequantize outputs tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>outputs</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>ValueError</strong> – </p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.TensorQuantFunction.symbolic">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">symbolic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">g</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trt_high_precision_dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'Float'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.TensorQuantFunction.symbolic" title="Link to this definition"></a></dt>
<dd><p>ONNX symbolic function.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.fake_quant_impl">
<span class="sig-name descname"><span class="pre">fake_quant_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.fake_quant_impl" title="Link to this definition"></a></dt>
<dd><p>Implementation of fake quantizing input according to number of bits.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Tensor</em>) – </p></li>
<li><p><strong>amax</strong> (<em>Tensor</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.quantize_op_abstract">
<span class="sig-name descname"><span class="pre">quantize_op_abstract</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">input</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">exponent_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">unsigned</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">narrow_range</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.quantize_op_abstract" title="Link to this definition"></a></dt>
<dd><p>Register an abstract implementation for quantizing tensor.</p>
<p>This abstract function returns an empty tensor with the same shape and dtype.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<em>Tensor</em>) – </p></li>
<li><p><strong>amax</strong> (<em>Tensor</em>) – </p></li>
<li><p><strong>num_bits</strong> (<em>int</em>) – </p></li>
<li><p><strong>exponent_bits</strong> (<em>int</em>) – </p></li>
<li><p><strong>unsigned</strong> (<em>bool</em>) – </p></li>
<li><p><strong>narrow_range</strong> (<em>bool</em>) – </p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.tensor_quant.scaled_e4m3_impl">
<span class="sig-name descname"><span class="pre">scaled_e4m3_impl</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">amax</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">disable_fused_kernel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.tensor_quant.scaled_e4m3_impl" title="Link to this definition"></a></dt>
<dd><p>Implementation of fake quantizing input to FP8.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Tensor</em>) – Torch tensor.</p></li>
<li><p><strong>amax</strong> (<em>Tensor</em>) – Absolute max range of the input tensor.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Input tensors faked quantized to FP8.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Tensor</em></p>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modelopt.torch.quantization.quant_modules.html" class="btn btn-neutral float-left" title="quant_modules" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modelopt.torch.quantization.utils.html" class="btn btn-neutral float-right" title="utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>