

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>model_quant &mdash; Model Optimizer 0.19.0</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=7a224f4b" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=20d3d275"></script>
      <script src="../../_static/tabs.js?v=3ee01567"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="nn" href="modelopt.torch.quantization.nn.html" />
    <link rel="prev" title="model_calib" href="modelopt.torch.quantization.model_calib.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            TensorRT Model Optimizer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/1_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/2_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/3_quantization.html">Quick Start: Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/4_pruning.html">Quick Start: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/5_distillation.html">Quick Start: Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/6_sparsity.html">Quick Start: Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/1_quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/2_pruning.html">Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/3_nas.html">NAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/4_distillation.html">Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/5_sparsity.html">Sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/6_save_load.html">Saving &amp; Restoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/7_speculative_decoding.html">Speculative Decoding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/1_tensorrt_llm_deployment.html">TensorRT-LLM Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples/0_all_examples.html">All GitHub Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/1_cifar_resnet.html">ResNet20 on CIFAR-10: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../examples/2_bert_prune_distill_quantize.html">HF BERT: Prune, Distill &amp; Quantize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_changelog.html">Model Optimizer Changelog</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../1_modelopt_api.html">modelopt API</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="modelopt.deploy.html">deploy</a></li>
<li class="toctree-l2"><a class="reference internal" href="modelopt.onnx.html">onnx</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="modelopt.torch.html">torch</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.distill.html">distill</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.export.html">export</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.nas.html">nas</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.opt.html">opt</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.prune.html">prune</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="modelopt.torch.quantization.html">quantization</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.algorithms.html">algorithms</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.calib.html">calib</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.config.html">config</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.conversion.html">conversion</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.export_onnx.html">export_onnx</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.extensions.html">extensions</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.mode.html">mode</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.model_calib.html">model_calib</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">model_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.nn.html">nn</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.optim.html">optim</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.plugins.html">plugins</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.qtensor.html">qtensor</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.quant_modules.html">quant_modules</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.tensor_quant.html">tensor_quant</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.torch.quantization.utils.html">utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.sparsity.html">sparsity</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.speculative.html">speculative</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.trace.html">trace</a></li>
<li class="toctree-l3"><a class="reference internal" href="modelopt.torch.utils.html">utils</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../support/1_contact.html">Contact us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support/2_faqs.html">FAQs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TensorRT Model Optimizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../1_modelopt_api.html">modelopt API</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.html">torch</a></li>
          <li class="breadcrumb-item"><a href="modelopt.torch.quantization.html">quantization</a></li>
      <li class="breadcrumb-item active">model_quant</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/reference/generated/modelopt.torch.quantization.model_quant.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="model-quant">
<h1>model_quant<a class="headerlink" href="#model-quant" title="Link to this heading"></a></h1>
<p id="module-modelopt.torch.quantization.model_quant">User-facing quantization API.</p>
<p class="rubric">Functions</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.quantize" title="modelopt.torch.quantization.model_quant.quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">quantize</span></code></a></p></td>
<td><p>Quantizes and calibrates the model in-place.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.auto_quantize" title="modelopt.torch.quantization.model_quant.auto_quantize"><code class="xref py py-obj docutils literal notranslate"><span class="pre">auto_quantize</span></code></a></p></td>
<td><p>API for <code class="docutils literal notranslate"><span class="pre">AutoQuantize</span></code> which quantizes a model by searching for the best quantization formats per-layer.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.disable_quantizer" title="modelopt.torch.quantization.model_quant.disable_quantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">disable_quantizer</span></code></a></p></td>
<td><p>Disable quantizer by wildcard or filter function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.enable_quantizer" title="modelopt.torch.quantization.model_quant.enable_quantizer"><code class="xref py py-obj docutils literal notranslate"><span class="pre">enable_quantizer</span></code></a></p></td>
<td><p>Enable quantizer by wildcard or filter function.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.print_quant_summary" title="modelopt.torch.quantization.model_quant.print_quant_summary"><code class="xref py py-obj docutils literal notranslate"><span class="pre">print_quant_summary</span></code></a></p></td>
<td><p>Print summary of all quantizer modules in the model.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.torch.quantization.model_quant.fold_weight" title="modelopt.torch.quantization.model_quant.fold_weight"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fold_weight</span></code></a></p></td>
<td><p>Fold weight quantizer for fast evaluation.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.auto_quantize">
<span class="sig-name descname"><span class="pre">auto_quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">constraints</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">{'effective_bits':</span> <span class="pre">4.8}</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">quantization_formats</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['W4A8_AWQ_BETA_CFG',</span> <span class="pre">'FP8_DEFAULT_CFG',</span> <span class="pre">None]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">data_loader</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_func</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_backward_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_calib_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">512</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_score_steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">128</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.auto_quantize" title="Link to this definition"></a></dt>
<dd><p>API for <code class="docutils literal notranslate"><span class="pre">AutoQuantize</span></code> which quantizes a model by searching for the best quantization formats per-layer.</p>
<p><code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> uses a gradient based sensitivity score to rank the per-layer quantization formats and search
for the best quantization formats per-layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>Module</em>) – A pytorch model with quantizer modules.</p></li>
<li><p><strong>constraints</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>float</em><em> | </em><em>str</em><em>]</em>) – <p>Constraints for the search. Currently we support only <code class="docutils literal notranslate"><span class="pre">effective_bits</span></code>.
<code class="docutils literal notranslate"><span class="pre">effective_bits</span></code> specifies the effective number of bits for the quantized model.</p>
<p>Here is an example for valid <code class="docutils literal notranslate"><span class="pre">effective_bits</span></code> argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># For an effective quantization bits of 4.8</span>
<span class="n">constraints</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;effective_bits&quot;</span><span class="p">:</span> <span class="mf">4.8</span><span class="p">}</span>
</pre></div>
</div>
</p></li>
<li><p><strong>quantization_formats</strong> (<em>List</em><em>[</em><em>str</em><em> | </em><em>None</em><em>]</em>) – <p>A list of the string names of the quantization formats to search for.
The supported quantization formats are as listed by <code class="xref py py-attr docutils literal notranslate"><span class="pre">modelopt.torch.quantization.config.choices</span></code>.</p>
<p>In addition, the quantization format can also be <code class="docutils literal notranslate"><span class="pre">None</span></code> which implies skipping quantization for
the layer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The quantization formats will be applied on a per-layer match basis. The global model level name
based quantizer attribute setting will be ignored. For example, in <code class="docutils literal notranslate"><span class="pre">FP8_DEFAULT_CFG</span></code> quantizer
configuration the key <code class="docutils literal notranslate"><span class="pre">&quot;*lm_head*&quot;:</span> <span class="pre">{&quot;enable&quot;:</span> <span class="pre">False}</span></code> disables quantization for the <code class="docutils literal notranslate"><span class="pre">lm_head</span></code>
layer. However in <code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code>, the quantization format for the <code class="docutils literal notranslate"><span class="pre">lm_head</span></code> layer will be searched.
This is because the key <code class="docutils literal notranslate"><span class="pre">&quot;*lm_head*&quot;</span></code> sets the quantizer attributes based on the global model level
name, not per-layer basis. The keys <code class="docutils literal notranslate"><span class="pre">&quot;*input_quantizer&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;*weight_quantizer&quot;</span></code> etc. in
<code class="docutils literal notranslate"><span class="pre">FP8_DEFAULT_CFG</span></code> match on a per-layer basis  - hence the corresponding quantizers
will be set as specified.</p>
</div>
<p>Here is an example <cite>quantization_formats</cite> argument:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># A valid `quantization_formats` argument</span>
<span class="c1"># This will search for the best per-layer quantization from FP8, W4A8_AWQ or No quantization</span>
<span class="n">quantization_formats</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;FP8_DEFAULT_CFG&quot;</span><span class="p">,</span> <span class="s2">&quot;W4A8_AWQ&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>
</pre></div>
</div>
</div></blockquote>
</p></li>
<li><p><strong>data_loader</strong> (<em>Iterable</em>) – An iterator that yields data that is to be used for calibrating quantized layers and estimating
<code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> scores.</p></li>
<li><p><strong>forward_step</strong> (<em>Callable</em><em>[</em><em>[</em><em>Module</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>Any</em><em> | </em><em>Tensor</em><em>]</em>) – <p>A callable that takes the model and a batch of data from <code class="docutils literal notranslate"><span class="pre">data_loader</span></code> as input, forwards
the data through the model and returns the model output.
This is a required argument.</p>
<p>Here is an example for a valid <code class="docutils literal notranslate"><span class="pre">forward_step</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Takes the model and a batch of data as input and returns the model output</span>
<span class="k">def</span> <span class="nf">forward_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
</p></li>
<li><p><strong>loss_func</strong> (<em>Callable</em><em>[</em><em>[</em><em>Any</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>Tensor</em><em>]</em>) – <p>(Optional) A callable that takes the model output and the batch of data as input and computes the
loss. The model output is the output given by <code class="docutils literal notranslate"><span class="pre">forward_step</span></code>. <cite>.backward()</cite> will be called on the loss.</p>
<p>Here is an example for a valid <code class="docutils literal notranslate"><span class="pre">loss_func</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Takes the model output and a batch of data as input and returns the loss</span>
<span class="k">def</span> <span class="nf">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="c1"># loss should be a scalar tensor such that loss.backward() can be called</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<p>If this argument is not provided, <code class="docutils literal notranslate"><span class="pre">forward_backward_step</span></code> should be provided.</p>
</p></li>
<li><p><strong>forward_backward_step</strong> (<em>Callable</em><em>[</em><em>[</em><em>Module</em><em>, </em><em>Any</em><em>]</em><em>, </em><em>Any</em><em>] </em><em>| </em><em>None</em>) – <p>(Optional) A callable that takes batch of data from <code class="docutils literal notranslate"><span class="pre">data_loader</span></code>, forwards it
through the model, computes the loss and runs backward on the loss.</p>
<p>Here is an example for a valid <code class="docutils literal notranslate"><span class="pre">forward_backward_step</span></code> argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Takes the model and a batch of data as input and runs forward and backward pass</span>
<span class="k">def</span> <span class="nf">forward_backward_step</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">my_loss_func</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">run_custom_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>If this argument is not provided, <code class="docutils literal notranslate"><span class="pre">loss_func</span></code> should be provided.</p>
</p></li>
<li><p><strong>num_calib_steps</strong> (<em>int</em>) – Number of batches to use for calibrating the quantized model. Suggested value is 512.</p></li>
<li><p><strong>num_score_steps</strong> (<em>int</em>) – Number of batches to use for estimating <code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> scores. Suggested value is 128.
A higher value could increase the time taken for performing <code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code>.</p></li>
<li><p><strong>verbose</strong> (<em>bool</em>) – If True, prints the search progress/intermediate results.</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Returns: A tuple (model, state_dict) where <code class="docutils literal notranslate"><span class="pre">model</span></code> is the searched and quantized model and</dt><dd><p><code class="docutils literal notranslate"><span class="pre">state_dict</span></code> contains the history and detailed stats of the search procedure.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> groups certain layers and restricts the quantization formats for them to be same. For example,
Q, K, V linear layers belonging to the same transformer layer will have the same quantization format.
This is to ensure compatibility with TensorRT-LLM which fuses these three linear layers into a single linear
layer.</p>
<p>A list of regex pattern rules as defined in <a class="reference internal" href="modelopt.torch.quantization.algorithms.html#modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.rules" title="modelopt.torch.quantization.algorithms.AutoQuantizeSearcher.rules"><code class="xref py py-attr docutils literal notranslate"><span class="pre">rules</span></code></a>
are used to specify the group of layers. The first captured group
in the regex pattern (i.e, <code class="docutils literal notranslate"><span class="pre">pattern.match(name).group(1)</span></code>) is used to group the layers. All the layers
that share the same first captured group will have the same quantization format..</p>
<p>For example, the rule <code class="docutils literal notranslate"><span class="pre">r&quot;^(.*?)\.(q_proj|k_proj|v_proj)$&quot;</span></code>
groups the <cite>q_proj</cite>, <cite>k_proj</cite>, <cite>v_proj</cite> linear layers belonging to the same transformer layer.</p>
<p>You may modify the rules to group the layers as per your requirement.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">modelopt.torch.quantization.algorithms</span> <span class="kn">import</span> <span class="n">AutoQuantizeSearcher</span>

<span class="c1"># To additionally group the layers belonging to same `mlp` layer,</span>
<span class="c1"># add the following rule</span>
<span class="n">AutoQuantizeSearcher</span><span class="o">.</span><span class="n">rules</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;^(.*?)\.mlp&quot;</span><span class="p">)</span>

<span class="c1"># Perform `auto_quantize`</span>
<span class="n">model</span><span class="p">,</span> <span class="n">state_dict</span> <span class="o">=</span> <span class="n">auto_quantize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> API and algorithm is experimental and subject to change. <code class="docutils literal notranslate"><span class="pre">auto_quantize</span></code> searched models
might not be readily deployable to TensorRT-LLM yet.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.disable_quantizer">
<span class="sig-name descname"><span class="pre">disable_quantizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wildcard_or_filter_func</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.disable_quantizer" title="Link to this definition"></a></dt>
<dd><p>Disable quantizer by wildcard or filter function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>Module</em>) – </p></li>
<li><p><strong>wildcard_or_filter_func</strong> (<em>str</em><em> | </em><em>Callable</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.enable_quantizer">
<span class="sig-name descname"><span class="pre">enable_quantizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">wildcard_or_filter_func</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.enable_quantizer" title="Link to this definition"></a></dt>
<dd><p>Enable quantizer by wildcard or filter function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>Module</em>) – </p></li>
<li><p><strong>wildcard_or_filter_func</strong> (<em>str</em><em> | </em><em>Callable</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.fold_weight">
<span class="sig-name descname"><span class="pre">fold_weight</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.fold_weight" title="Link to this definition"></a></dt>
<dd><p>Fold weight quantizer for fast evaluation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>Module</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.print_quant_summary">
<span class="sig-name descname"><span class="pre">print_quant_summary</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.print_quant_summary" title="Link to this definition"></a></dt>
<dd><p>Print summary of all quantizer modules in the model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>model</strong> (<em>Module</em>) – </p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.torch.quantization.model_quant.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward_loop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.torch.quantization.model_quant.quantize" title="Link to this definition"></a></dt>
<dd><p>Quantizes and calibrates the model in-place.</p>
<p>This method performs replacement of modules with their quantized counterparts and
performs calibration as specified by <code class="docutils literal notranslate"><span class="pre">quant_cfg</span></code>.
<code class="docutils literal notranslate"><span class="pre">forward_loop</span></code> is used to forward data through the model and gather statistics for calibration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>model</strong> (<em>Module</em>) – A pytorch model</p></li>
<li><p><strong>config</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – <p>A dictionary or an instance of
<a class="reference internal" href="modelopt.torch.quantization.config.html#modelopt.torch.quantization.config.QuantizeConfig" title="modelopt.torch.quantization.config.QuantizeConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizeConfig</span></code></a> specifying the
values for keys <code class="docutils literal notranslate"><span class="pre">&quot;quant_cfg&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;algorithm&quot;</span></code>.
It is basically a dictionary specifying the values for keys <code class="docutils literal notranslate"><span class="pre">&quot;quant_cfg&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;algorithm&quot;</span></code>.
The <code class="docutils literal notranslate"><span class="pre">&quot;quant_cfg&quot;</span></code> key specifies the quantization configurations.
The <code class="docutils literal notranslate"><span class="pre">&quot;algorithm&quot;</span></code> key specifies the <code class="docutils literal notranslate"><span class="pre">algorithm</span></code> argument to
<a class="reference internal" href="modelopt.torch.quantization.model_calib.html#modelopt.torch.quantization.model_calib.calibrate" title="modelopt.torch.quantization.model_calib.calibrate"><code class="xref py py-meth docutils literal notranslate"><span class="pre">calibrate</span></code></a>.</p>
<p>Quantization configurations is a dictionary mapping wildcards or filter functions
to its quantizer attributes. The wildcards or filter functions  are matched
against the quantizer module names. The quantizer modules have names ending with
<code class="docutils literal notranslate"><span class="pre">weight_quantizer</span></code> and <code class="docutils literal notranslate"><span class="pre">input_quantizer</span></code> and they perform weight quantization and
input quantization (or activation quantization) respectively. The quantizer modules
are instances of
<a class="reference internal" href="modelopt.torch.quantization.nn.modules.tensor_quantizer.html#modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer" title="modelopt.torch.quantization.nn.modules.tensor_quantizer.TensorQuantizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorQuantizer</span></code></a>.
The quantizer attributes are defined by <code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerAttributeConfig</span></code>. See
<code class="xref py py-class docutils literal notranslate"><span class="pre">QuantizerAttributeConfig</span></code> for details on the quantizer attributes and their values.</p>
<p>An example <code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary is given below:</p>
<p>See <a class="reference internal" href="modelopt.torch.quantization.config.html#quantization-formats"><span class="std std-ref">Quantization Formats</span></a> to learn more about the supported
quantization formats. See <a class="reference internal" href="modelopt.torch.quantization.config.html#quantization-configs"><span class="std std-ref">Quantization Configs</span></a> for more details on
<code class="docutils literal notranslate"><span class="pre">config</span></code> dictionary.</p>
</p></li>
<li><p><strong>forward_loop</strong> (<em>Callable</em><em>[</em><em>[</em><em>Module</em><em>]</em><em>, </em><em>None</em><em>] </em><em>| </em><em>None</em>) – <p>A callable that forwards all calibration data through the model. This is used
to gather statistics for calibration. It should take model as the argument. It does not need
to return anything.</p>
<p>This argument is not required for weight-only quantization with the <code class="docutils literal notranslate"><span class="pre">&quot;max&quot;</span></code>
algorithm.</p>
<p>Here are a few examples for correct <code class="docutils literal notranslate"><span class="pre">forward_loop</span></code> definitions:
Example 1:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># iterate over the data loader and forward data through the model</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
        <span class="n">model</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
</pre></div>
</div>
<p>Example 2:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="c1"># evaluate the model on the task</span>
    <span class="k">return</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">task</span><span class="p">,</span> <span class="o">....</span><span class="p">)</span>
</pre></div>
</div>
<p>Example 3:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">forward_loop</span><span class="p">(</span><span class="n">model</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="c1"># run evaluation pipeline</span>
    <span class="n">evaluator</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="n">evaluator</span><span class="o">.</span><span class="n">evaluate</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Calibration does not require forwarding the entire dataset through the model.
Please subsample the dataset or reduce the number of batches if needed.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><em>Module</em></p>
</dd>
</dl>
<p>Returns: A pytorch model which has been quantized and calibrated.</p>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modelopt.torch.quantization.model_calib.html" class="btn btn-neutral float-left" title="model_calib" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modelopt.torch.quantization.nn.html" class="btn btn-neutral float-right" title="nn" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>