<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>model_config_trt &mdash; Model Optimizer 0.11.2</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../../_static/autodoc_pydantic.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d10054b6" />


  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->

        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=5929fcd5"></script>
        <script src="../../_static/doctools.js?v=888ff710"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
        <script src="../../_static/copybutton.js?v=20d3d275"></script>
        <script src="../../_static/tabs.js?v=3ee01567"></script>
        <script>let toggleHintShow = 'Click to show';</script>
        <script>let toggleHintHide = 'Click to hide';</script>
        <script>let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="nemo_utils" href="modelopt.deploy.llm.nemo_utils.html" />
    <link rel="prev" title="generate" href="modelopt.deploy.llm.generate.html" />
</head>

<body class="wy-body-for-nav">
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >



          <a href="../../index.html" class="icon icon-home">
            TensorRT Model Optimizer
          </a>
              <div class="version">
                0.11.2
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/1_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/2_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/3_quantization.html">Quick Start: Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/6_sparsity.html">Quick Start: Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Optimization Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../guides/1_quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../guides/5_sparsity.html">Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../deployment/1_tensorrt_llm_deployment.html">TensorRT-LLM Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples/0_all_examples.html">All ModelOpt Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../0_versions.html">Model Optimizer Changelog</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../1_modelopt_api.html">modelopt API</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="modelopt.deploy.html">deploy</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="modelopt.deploy.llm.html">llm</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="modelopt.deploy.llm.generate.html">generate</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">model_config_trt</a></li>
<li class="toctree-l4"><a class="reference internal" href="modelopt.deploy.llm.nemo_utils.html">nemo_utils</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="modelopt.onnx.html">onnx</a></li>
<li class="toctree-l2"><a class="reference internal" href="modelopt.torch.html">torch</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../support/1_contact.html">Contact us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../support/2_faqs.html">FAQs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">TensorRT Model Optimizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../1_modelopt_api.html">modelopt API</a></li>
          <li class="breadcrumb-item"><a href="modelopt.deploy.html">deploy</a></li>
          <li class="breadcrumb-item"><a href="modelopt.deploy.llm.html">llm</a></li>
      <li class="breadcrumb-item active">model_config_trt</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/reference/generated/modelopt.deploy.llm.model_config_trt.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">

  <section id="model-config-trt">
<h1>model_config_trt<a class="headerlink" href="#model-config-trt" title="Link to this heading"></a></h1>
<p id="module-modelopt.deploy.llm.model_config_trt">The API convert the TensorRT-LLM checkpoint to the engines.</p>
<p class="rubric">Functions</p>
<table class="autosummary longtable docutils align-default">
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#modelopt.deploy.llm.model_config_trt.build_tensorrt_llm" title="modelopt.deploy.llm.model_config_trt.build_tensorrt_llm"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build_tensorrt_llm</span></code></a></p></td>
<td><p>The API to convert the TensorRT-LLM checkpoint to engines.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#modelopt.deploy.llm.model_config_trt.build_tensorrt_llm_rank" title="modelopt.deploy.llm.model_config_trt.build_tensorrt_llm_rank"><code class="xref py py-obj docutils literal notranslate"><span class="pre">build_tensorrt_llm_rank</span></code></a></p></td>
<td><p>The API to convert the TensorRT-LLM checkpoint to the engine for a single rank.</p></td>
</tr>
</tbody>
</table>
<dl class="py function">
<dt class="sig sig-object py" id="modelopt.deploy.llm.model_config_trt.build_tensorrt_llm">
<span class="sig-name descname"><span class="pre">build_tensorrt_llm</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_input_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_output_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_beam_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_build_workers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_prompt_embedding_table_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.deploy.llm.model_config_trt.build_tensorrt_llm" title="Link to this definition"></a></dt>
<dd><p>The API to convert the TensorRT-LLM checkpoint to engines.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_config</strong> (<em>str</em><em> | </em><em>Path</em>) – The pretrained_config (file path) exported by
<code class="docutils literal notranslate"><span class="pre">modelopt.torch.export.export_tensorrt_llm_checkpoint</span></code>.</p></li>
<li><p><strong>engine_dir</strong> (<em>str</em><em> | </em><em>Path</em>) – The target output directory to save the built tensorrt_llm engines.</p></li>
<li><p><strong>max_input_len</strong> (<em>int</em>) – The max input sequence length.</p></li>
<li><p><strong>max_output_len</strong> (<em>int</em>) – The max output sequence length.</p></li>
<li><p><strong>max_batch_size</strong> (<em>int</em>) – The max batch size.</p></li>
<li><p><strong>max_beam_width</strong> (<em>int</em>) – The max beam search width.</p></li>
<li><p><strong>max_num_tokens</strong> (<em>int</em><em> | </em><em>None</em>) – The max number of tokens that can be processed at the same time.
For the context phase, the max_num_tokens counts the full sequence length.
For the generation phase, the max_num_tokens counts only the ones under generation
as the input sequence has been processed as cached.
max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
when inflight batching is enabled.
Higher max_num_tokens means more GPU memory will be used for resource allocation.
If not specified the max_num_tokens will be set to the max bound.
Details: <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md" rel="noopener noreferrer" target="_blank">https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md</a></p></li>
<li><p><strong>num_build_workers</strong> (<em>int</em>) – The number of workers to use for the building process.
If build time is a concern, you can increase this worker count to num of GPUs.
At a lost of higer CPU memory usage footprint.
If CPU memory is limited, num_build_workers should be set to 1 to conserve memory.</p></li>
<li><p><strong>enable_sparsity</strong> (<em>bool</em>) – The switch to enable sparsity for TRT compiler.
With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
weight tensors are sparsified. This increases engine building time significantly.</p></li>
<li><p><strong>max_prompt_embedding_table_size</strong> (<em>int</em>) – Length of the prepended/concatenated embeddings (either multimodal
feature embeddings or prompt tuning embeddings) to the LLM input embeddings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="modelopt.deploy.llm.model_config_trt.build_tensorrt_llm_rank">
<span class="sig-name descname"><span class="pre">build_tensorrt_llm_rank</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pretrained_config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weights</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine_dir</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_input_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_output_len</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">200</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_beam_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_num_tokens</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_sparsity</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_prompt_embedding_table_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#modelopt.deploy.llm.model_config_trt.build_tensorrt_llm_rank" title="Link to this definition"></a></dt>
<dd><p>The API to convert the TensorRT-LLM checkpoint to the engine for a single rank.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>pretrained_config</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Any</em><em>]</em>) – The pretrained_config (dict) exported by
<code class="docutils literal notranslate"><span class="pre">modelopt.torch.export.torch_to_tensorrt_llm_checkpoint</span></code>.</p></li>
<li><p><strong>weights</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>Tensor</em><em>]</em>) – a dict of model weights and scaling factors.
If not provided, the weights will be loaded from the directory of the pretrained_config.</p></li>
<li><p><strong>rank</strong> (<em>int</em>) – the GPU rank of the engine to build.</p></li>
<li><p><strong>engine_dir</strong> (<em>str</em><em> | </em><em>Path</em>) – The target output directory to save the built tensorrt_llm engines.</p></li>
<li><p><strong>max_input_len</strong> (<em>int</em>) – The max input sequence length.</p></li>
<li><p><strong>max_output_len</strong> (<em>int</em>) – The max output sequence length.</p></li>
<li><p><strong>max_batch_size</strong> (<em>int</em>) – The max batch size.</p></li>
<li><p><strong>max_beam_width</strong> (<em>int</em>) – The max beam search width.</p></li>
<li><p><strong>max_num_tokens</strong> (<em>int</em><em> | </em><em>None</em>) – The max number of tokens that can be processed at the same time.
For the context phase, the max_num_tokens counts the full sequence length.
For the generation phase, the max_num_tokens counts only the ones under generation
as the input sequence has been processed as cached.
max_num_tokens should fall between [max_batch_size * max_beam_width, max_batch_size * max_input_len].
when inflight batching is enabled.
Higher max_num_tokens means more GPU memory will be used for resource allocation.
If not specified the max_num_tokens will be set to the max bound.
Details: <a class="reference external" href="https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md" rel="noopener noreferrer" target="_blank">https://github.com/NVIDIA/TensorRT-LLM/blob/main/docs/source/perf_best_practices.md</a></p></li>
<li><p><strong>enable_sparsity</strong> (<em>bool</em>) – The switch to enable sparsity for TRT compiler.
With this flag, the TRT compiler will search tactics of sparse kernels for each node of which
weight tensors are sparsified. This increases engine building time significantly.</p></li>
<li><p><strong>max_prompt_embedding_table_size</strong> (<em>int</em>) – Length of the prepended/concatenated embeddings (either multimodal
feature embeddings or prompt tuning embeddings) to the LLM input embeddings.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="modelopt.deploy.llm.generate.html" class="btn btn-neutral float-left" title="generate" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modelopt.deploy.llm.nemo_utils.html" class="btn btn-neutral float-right" title="nemo_utils" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

</body>
</html>
