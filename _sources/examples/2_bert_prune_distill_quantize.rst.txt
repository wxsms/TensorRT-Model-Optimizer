HF BERT: Prune, Distill & Quantize
==================================

This example shows how to compress a `Hugging Face Bert large model for Question Answering <https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad>`_
using the combination of ``modelopt.torch.prune``, ``modelopt.torch.distill`` and ``modelopt.torch.quantize``. More specifically, we will:

#. Prune the Bert large model to 50% FLOPs with GradNAS algorithm and fine-tune with distillation
#. Quantize the fine-tuned model to INT8 precision with Post-Training Quantization (PTQ) and Quantize Aware Training (QAT) with distillation
#. Export the quantized model to ONNX format for deployment with TensorRT

Prerequisites
-------------
#. Install the Model Optimizer and optional torch and huggingface dependencies:

    .. code-block:: bash

        pip install "nvidia-modelopt[torch,hf]" --extra-index-url https://pypi.nvidia.com

.. note::
    This example has been tested on 8 x 24GB A5000 GPUs with PyTorch 2.4 and CUDA 12.4. It takes
    about 2 hours to complete all the stages of the optimization. Most of the time is spent
    on fine-tuning and QAT.

Full code
---------
You can view the full code below with ModelOpt integration points highlighted.
The source code and scripts are also available on `ModelOpt GitHub <https://github.com/NVIDIA/TensorRT-Model-Optimizer/tree/main/chained_optimizations>`_

.. NOTE: MAKE SURE THAT LINE NUMBERS ARE UP-TO-DATE

.. toggle::

    .. literalinclude:: ../../../examples/public/chained_optimizations/bert_prune_distill_quantize.py
        :linenos:
        :emphasize-lines: 74-78, 243-288, 872-884, 1040-1044, 1142-1155, 1157-1182, 1184-1204, 1207-1215, 1228-1232
        :caption: bert_prune_distill_quantize.py

Commands
--------

#. First we prune the Bert large model to 50% FLOPs with GradNAS algorithm. Then, we fine-tune the pruned
   model with distillation from unpruned teacher model to recover 99+% of the initial F1 score (93.15).
   We recommend using multiple GPUs for fine-tuning. Note that we use more epochs
   for fine-tuning, which is different from the 2 epochs used originally in fine-tuning Bert without distillation since
   distillation requires more epochs to converge but achieves much better results.

    .. literalinclude:: ../../../examples/public/chained_optimizations/scripts/1_prune.sh
        :language: bash
        :caption: 1_prune.sh

#. Quantize the fine-tuned model to INT8 precision and run calibration (PTQ).
   Note that PTQ will result in a slight drop in F1 score but we will be able to recover the F1 score with QAT.
   We run QAT with distillation as well from unpruned teacher model.

    .. literalinclude:: ../../../examples/public/chained_optimizations/scripts/2_int8_quantize.sh
        :language: bash
        :caption: 2_int8_quantize.sh

#. Export the quantized model to ONNX format for deployment with TensorRT.

    .. literalinclude:: ../../../examples/public/chained_optimizations/scripts/3_onnx_export.sh
        :language: bash
        :caption: 3_onnx_export.sh
