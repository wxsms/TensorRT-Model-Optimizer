.. From https://github.com/sphinx-doc/sphinx/blob/5.x/sphinx/ext/autosummary/templates/autosummary/module.rst

layer\_utils
============

.. List the submodules





.. Autodoc anything defined in the module itself

   TODO: WE DON'T USE THIS OPTION RIGHT NOW BUT WE CAN REACTIVATE IF WANTED
   We use :ignore-module-all: so sphinx does not document the same module twice, even if it is reimported
   For reimports that should be documented somewhere other than where they are defined, the re-imports
   __module__ should be manually overridden -- i.e. in the ``__init__.py`` which contains ``from xxx import YYY``,
   add in ``YYY.__module__ = __name__``.

.. automodule:: modelopt.torch.export.layer_utils
   :members:
   :undoc-members:

   .. Also show members without docstrings. Only members from __all__ are considered as per conf.py
   .. Ideally we should add docstrings for these members.


   .. Overview table of available classes in the module
   
   
   


   .. Overview table of available functions in the module
   
   
   .. rubric:: Functions

   .. autosummary::
      :nosignatures:
   
      build_attention_config
      build_conv_config
      build_decoder_config
      build_embedding_config
      build_layernorm_config
      build_linear_config
      build_medusa_heads_config
      build_mlp_config
      build_moe_config
      build_qkv
      build_recurrent_config
      build_stacked_experts
      check_model_compatibility
      get_activation_scaling_factor
      get_kv_cache_dtype
      get_kv_cache_scaling_factor
      get_prequant_scaling_factor
      get_qkv_and_avg_prequant_scale
      get_quantization_format
      get_scaling_factor
      get_transformer_layers
      get_weight_block_size
      get_weight_scaling_factor
      get_weight_scaling_factor_2
      is_attention
      is_decoder_list
      is_embedding
      is_layernorm
      is_linear
      is_mlp
      is_moe
      is_quantlinear
      is_recurrent
   
   