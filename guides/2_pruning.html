

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Pruning &mdash; Model Optimizer 0.19.0</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
      <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
      <link rel="stylesheet" type="text/css" href="../_static/autodoc_pydantic.css" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=7a224f4b" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=5929fcd5"></script>
      <script src="../_static/doctools.js?v=888ff710"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=20d3d275"></script>
      <script src="../_static/tabs.js?v=3ee01567"></script>
      <script>let toggleHintShow = 'Click to show';</script>
      <script>let toggleHintHide = 'Click to hide';</script>
      <script>let toggleOpenOnPrint = 'true';</script>
      <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script src="https://cdn.jsdelivr.net/npm/@jupyter-widgets/html-manager@^1.0.1/dist/embed-amd.js"></script>
      <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NAS" href="3_nas.html" />
    <link rel="prev" title="ONNX Quantization (Beta)" href="_onnx_quantization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            TensorRT Model Optimizer
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/1_overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/2_installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/3_quantization.html">Quick Start: Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/4_pruning.html">Quick Start: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/5_distillation.html">Quick Start: Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/6_sparsity.html">Quick Start: Sparsity</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="1_quantization.html">Quantization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pruning</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pruning-and-subnet-search">Pruning and subnet search</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisites">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#perform-pruning">Perform pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#storing-the-prune-results">Storing the prune results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#customizing-pruning-config">Customizing pruning config</a></li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-the-search-space-and-choosing-constraints">Profiling the search space and choosing constraints</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#fine-tuning">Fine-tuning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Prerequisites</a></li>
<li class="toctree-l3"><a class="reference internal" href="#load-the-pruned-model">Load the pruned model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#run-fine-tuning">Run fine-tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy">Deploy</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#pruning-concepts">Pruning Concepts</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="3_nas.html">NAS</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_distillation.html">Distillation</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_sparsity.html">Sparsity</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_save_load.html">Saving &amp; Restoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_speculative_decoding.html">Speculative Decoding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deployment</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deployment/1_tensorrt_llm_deployment.html">TensorRT-LLM Deployment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/0_all_examples.html">All GitHub Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/1_cifar_resnet.html">ResNet20 on CIFAR-10: Pruning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/2_bert_prune_distill_quantize.html">HF BERT: Prune, Distill &amp; Quantize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/0_changelog.html">Model Optimizer Changelog</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/1_modelopt_api.html">modelopt API</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../support/1_contact.html">Contact us</a></li>
<li class="toctree-l1"><a class="reference internal" href="../support/2_faqs.html">FAQs</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">TensorRT Model Optimizer</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Pruning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/guides/2_pruning.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pruning">
<h1>Pruning<a class="headerlink" href="#pruning" title="Link to this heading"></a></h1>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Checkout <a class="reference internal" href="../examples/1_cifar_resnet.html"><span class="doc">ResNet20 on CIFAR-10 Notebook</span></a> and
<a class="reference internal" href="../examples/2_bert_prune_distill_quantize.html"><span class="doc">HF BERT Prune, Distill &amp; Quantize</span></a>
for an end-to-end example of pruning.</p>
</div>
<p>ModelOpt provides three main pruning methods (aka <code class="docutils literal notranslate"><span class="pre">mode</span></code>) - Minitron, FastNAS and GradNAS - via a unified API
<a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune</span></code></a>. Given a model,
these methods finds the subnet which meets the given deployment constraints (e.g. FLOPs, parameters)
from your provided base model with little to no accuracy degradation (depending on how aggressive is the pruning).
These pruning methods support pruning the convolutional and linear layers, and
attention heads of the model. More details on these pruning modes is as follows:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">fastnas</span></code>: A pruning method recommended for Computer Vision models. Given a pretrained model,
FastNAS finds the subnet which maximizes the score function while meeting the given constraints.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mcore_gpt_minitron</span></code>: A pruning method developed by NVIDIA Research for pruning GPT-style models (e.g. Llama 3)
in NVIDIA NeMo or Megatron-LM framework that are using Pipeline Parallellism. It uses the activation
magnitudes to prune the mlp, attention heads, and GQA query groups. Checkout more details of the
algorithm in the <a class="reference external" href="https://arxiv.org/abs/2408.11796" rel="noopener noreferrer" target="_blank">paper</a>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gradnas</span></code>: A light-weight pruning method recommended for language models like Hugging Face BERT and GPT-J.
It uses the gradient information to prune the model’s linear layers and attention heads to meet the given constraints.</p></li>
</ol>
<p>Follow the steps described below to obtain the optimal model satisfying your
requirements using <a class="reference internal" href="../reference/generated/modelopt.torch.prune.html#module-modelopt.torch.prune" title="modelopt.torch.prune"><code class="xref py py-mod docutils literal notranslate"><span class="pre">mtp</span></code></a>:</p>
<ol class="arabic simple">
<li><p><strong>Training</strong>: Simply train your model using existing training pipeline or load a pre-trained
checkpoint for your model.</p></li>
<li><p><strong>Pruning</strong>: Prune the model using our provided <a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune</span></code></a>
API and get an optimal subnet describing the pruned network architecture.</p></li>
<li><p><strong>Fine-tuning</strong>: fine-tune the resulting subnet to recover the accuracy.</p></li>
</ol>
<p><em>To find out more about the concepts behind NAS and pruning, please refer to</em>
<a class="reference internal" href="3_nas.html#nas-concepts"><span class="std std-ref">NAS concepts</span></a>.</p>
<section id="training">
<h2>Training<a class="headerlink" href="#training" title="Link to this heading"></a></h2>
<p>To perform pruning, you can either use a model obtained by converting a pre-trained
checkpoint model or train the model from scratch.</p>
<div class="tab-set docutils container">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Use a pre-trained checkpoint</label><div class="tab-content docutils container">
<p>Simply initialize your model and load the checkpoint before you start using ModelOpt.</p>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Train the model</label><div class="tab-content docutils container">
<p>You can simply use your existing training pipeline to train the model without
further modifications.</p>
</div>
</div>
</section>
<section id="pruning-and-subnet-search">
<span id="pruning-search"></span><h2>Pruning and subnet search<a class="headerlink" href="#pruning-and-subnet-search" title="Link to this heading"></a></h2>
<p>The next step in pruning is to perform a search over potential subnet architectures, i.e., prune the
network, to find the best subnet satisfying your deployment constraints.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>To perform pruning (<a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune()</span></code></a>) on a trained
model, you need to set up data loaders, provide search <code class="docutils literal notranslate"><span class="pre">constraints</span></code> and a <code class="docutils literal notranslate"><span class="pre">dummy_input</span></code> (to measure
your deployment constraints).</p></li>
<li><p>You can provide one search constraint for either <code class="docutils literal notranslate"><span class="pre">flops</span></code> or <code class="docutils literal notranslate"><span class="pre">params</span></code> by
specifying an upper bound in terms of absolute number (<code class="docutils literal notranslate"><span class="pre">3e-6</span></code>) or a percentage (<code class="docutils literal notranslate"><span class="pre">&quot;60%&quot;</span></code>).</p></li>
<li><p>You should also specify the pruning algorithm (<code class="docutils literal notranslate"><span class="pre">mode</span></code>), you would like to use. Depending on the
mode, you will need to provide additional <code class="docutils literal notranslate"><span class="pre">config</span></code> parameters like <code class="docutils literal notranslate"><span class="pre">score_func</span></code> (<code class="docutils literal notranslate"><span class="pre">fastnas</span></code> mode)
or <code class="docutils literal notranslate"><span class="pre">loss_func</span></code> (<code class="docutils literal notranslate"><span class="pre">gradnas</span></code> mode), <code class="docutils literal notranslate"><span class="pre">dataloader</span></code>, <code class="docutils literal notranslate"><span class="pre">checkpoint</span></code>, etc. The most common score function
is the validation accuracy of the model and is used to rank the sub-nets sampled from the serach space.
Loss function is used to run some forward and backward passes on the train dataloader to get the gradients.</p></li>
<li><p>Please see the API reference of <a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune()</span></code></a> for more details.</p></li>
</ol>
<p>Below we show an example using <a class="reference internal" href="../reference/generated/modelopt.torch.prune.mode.html#modelopt.torch.prune.mode.FastNASModeDescriptor" title="modelopt.torch.prune.mode.FastNASModeDescriptor"><code class="xref py py-class docutils literal notranslate"><span class="pre">&quot;fastnas&quot;</span></code></a>.</p>
</section>
<section id="perform-pruning">
<h3>Perform pruning<a class="headerlink" href="#perform-pruning" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">modelopt.torch.prune</span> <span class="k">as</span> <span class="nn">mtp</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="c1"># User-defined model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">()</span>

<span class="c1"># Load pretrained weights here</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">pretrained_weights</span><span class="p">)</span>


<span class="c1"># Wrap your original validation function to only take the model as input.</span>
<span class="c1"># This function acts as the score function to rank models.</span>
<span class="k">def</span> <span class="nf">score_func</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>


<span class="c1"># Define a dummy input with similar shape as that of your input data</span>
<span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">244</span><span class="p">)</span>

<span class="c1"># Prune to a model with less than or equal to 60% of original FLOPs</span>
<span class="n">prune_constraints</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;flops&quot;</span><span class="p">:</span> <span class="s2">&quot;60%&quot;</span><span class="p">}</span>

<span class="c1"># prune_res (dict) contains state_dict / stats of the pruner/searcher.</span>
<span class="n">pruned_model</span><span class="p">,</span> <span class="n">prune_res</span> <span class="o">=</span> <span class="n">mtp</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;fastnas&quot;</span><span class="p">,</span>
    <span class="n">constraints</span><span class="o">=</span><span class="n">prune_constraints</span><span class="p">,</span>
    <span class="n">dummy_input</span><span class="o">=</span><span class="n">dummy_input</span><span class="p">,</span>
    <span class="n">config</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;data_loader&quot;</span><span class="p">:</span> <span class="n">train_loader</span><span class="p">,</span>  <span class="c1"># training data is used for calibrating BN layers</span>
        <span class="s2">&quot;score_func&quot;</span><span class="p">:</span> <span class="n">score_func</span><span class="p">,</span>  <span class="c1"># validation score is used to rank the subnets</span>
        <span class="c1"># checkpoint to store the search state and resume or re-run the search with different constraint</span>
        <span class="s2">&quot;checkpoint&quot;</span><span class="p">:</span> <span class="s2">&quot;modelopt_fastnas_search_checkpoint.pth&quot;</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Note that during pruning we first convert the model into a search space containing different
possible network configurations and an optimal configuration is then searched for.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If the runtime of the score function is longer than a few minutes, consider subsampling the
dataset used in the score function. A PyTorch dataset can be subsampled using
<a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset" rel="noopener noreferrer" target="_blank">torch.utils.data.Subset</a>
as following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">subset_dataset</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Subset</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">indices</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pruning will modify the model in-place.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune()</span></code></a> supports distributed data parallelism
via <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> in PyTorch.</p>
<p>Currently, the API does not support pruning pytorch Fully Sharded Data Parallel (FSDP) models
so you would need to run pruning on a CPU and then finetune using FSDP. Note that GradNAS is
much much faster than FastNAS (hence feasible on CPU as well) and is recommended for
language models like BERT, GPT-J 6B, and other LLMs.</p>
</div>
</section>
<section id="storing-the-prune-results">
<h3>Storing the prune results<a class="headerlink" href="#storing-the-prune-results" title="Link to this heading"></a></h3>
<p>To store the pruned model for future use you can use
<a class="reference internal" href="../reference/generated/modelopt.torch.opt.conversion.html#modelopt.torch.opt.conversion.save" title="modelopt.torch.opt.conversion.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mto.save()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">modelopt.torch.opt</span> <span class="k">as</span> <span class="nn">mto</span>

<span class="n">mto</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">,</span> <span class="s2">&quot;modelopt_pruned_model.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please see <a class="reference internal" href="6_save_load.html#save-restore"><span class="std std-ref">saving and restoring of ModelOpt-modified models</span></a> to learn
about all the available options for saving and restoring.</p>
</div>
</section>
<section id="customizing-pruning-config">
<h3>Customizing pruning config<a class="headerlink" href="#customizing-pruning-config" title="Link to this heading"></a></h3>
<p>In the above example, we have used the default mode config for <code class="docutils literal notranslate"><span class="pre">mtp.prune()</span></code>. You can see it using
<a class="reference internal" href="../reference/generated/modelopt.torch.prune.config.html#modelopt.torch.prune.config.FastNASConfig" title="modelopt.torch.prune.config.FastNASConfig"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.config.FastNASConfig()</span></code></a>.
You can also specify custom mode configs to have a different search space. See
<a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune()</span></code></a> documentation for more information. An
example config is shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">modelopt.torch.prune</span> <span class="k">as</span> <span class="nn">mtp</span>

<span class="c1"># config to restrict the search space to have a Conv2d out channels as multiple of 64</span>
<span class="n">ss_config</span> <span class="o">=</span> <span class="n">mtp</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">FastNASConfig</span><span class="p">()</span>
<span class="n">ss_config</span><span class="p">[</span><span class="s2">&quot;nn.Conv2d&quot;</span><span class="p">][</span><span class="s2">&quot;*&quot;</span><span class="p">][</span><span class="s2">&quot;channel_divisor&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">64</span>

<span class="c1"># run pruning as shown above</span>
<span class="n">mtp</span><span class="o">.</span><span class="n">prune</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="p">[(</span><span class="s2">&quot;fastnas&quot;</span><span class="p">,</span> <span class="n">ss_config</span><span class="p">)],</span> <span class="o">...</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="profiling-the-search-space-and-choosing-constraints">
<span id="fastnas-profile"></span><h3>Profiling the search space and choosing constraints<a class="headerlink" href="#profiling-the-search-space-and-choosing-constraints" title="Link to this heading"></a></h3>
<p>The search space describes the candidates of potential pruned subnets. You can obtain information
about the overall statistics of the search space in <a class="reference internal" href="../reference/generated/modelopt.torch.prune.pruning.html#modelopt.torch.prune.pruning.prune" title="modelopt.torch.prune.pruning.prune"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mtp.prune()</span></code></a> API.
Following info will be printed before the pruning process is started:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    Profiling the following subnets from the given model: (&#39;min&#39;, &#39;centroid&#39;, &#39;max&#39;).
--------------------------------------------------------------------------------

                            Profiling Results
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Constraint   ┃ min          ┃ centroid     ┃ max          ┃ max/min ratio ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ flops        │ 274.34M      │ 1.28G        │ 4.59G        │ 16.73         │
│ params       │ 2.70M        │ 9.75M        │ 25.50M       │ 9.43          │
└──────────────┴──────────────┴──────────────┴──────────────┴───────────────┘

            Constraints Evaluation
┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓
┃              ┃              ┃ Satisfiable  ┃
┃ Constraint   ┃ Upper Bound  ┃ Upper Bound  ┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩
│ flops        │ 2.75G        │ True         │
└──────────────┴──────────────┴──────────────┘


Search Space Summary:
----------------------------------------------------------------------------------------------------
* conv1.out_channels                                                               [32, 64]
  conv1.in_channels                                                                [3]
  bn1.num_features                                                                 [32, 64]
* layer1.0.conv1.out_channels                                                      [32, 64]
  layer1.0.conv1.in_channels                                                       [32, 64]
  layer1.0.bn1.num_features                                                        [32, 64]
* layer1.0.conv2.out_channels                                                      [32, 64]
  ...
  ...
  ...
* layer4.2.conv1.out_channels                                                      [32, 64, 96, 128, ..., 416, 448, 480, 512]
  layer4.2.conv1.in_channels                                                       [2048]
  layer4.2.bn1.num_features                                                        [32, 64, 96, 128, ..., 416, 448, 480, 512]
* layer4.2.conv2.out_channels                                                      [32, 64, 96, 128, ..., 416, 448, 480, 512]
  layer4.2.conv2.in_channels                                                       [32, 64, 96, 128, ..., 416, 448, 480, 512]
  layer4.2.bn2.num_features                                                        [32, 64, 96, 128, ..., 416, 448, 480, 512]
  layer4.2.conv3.out_channels                                                      [2048]
  layer4.2.conv3.in_channels                                                       [32, 64, 96, 128, ..., 416, 448, 480, 512]
----------------------------------------------------------------------------------------------------
Number of configurable hparams: 36
Total size of the search space: 2.48e+28
Note: all constraints can be satisfied within the search space!
</pre></div>
</div>
<p>The profiling results will help you understand the search space and come up with a potential search
constraint that you can iterate on.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Generally a search space with max/min ratio above 3 is a good search space with many layers pruneable.
The higher the max/min ratio, the more pruneable the model is (potentially making FastNAS slower but better).</p>
<p>A good starting point for your search constraints is the centroid of the search space. If you are using flops/params
constraints, we highly recommend you first obtain a pruned model and measure its latency on your target deployment
before you finetune the pruned model. Depending on the latency, you can adjust the constraints accordingly.
Once you have a model that is within your latency constraints, you can fine-tune it to recover the accuracy.
If you are unable to recover the accuracy (perhaps because of too aggressive pruning), you can try increasing
the constraints and repeat the process.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the constraint cannnot be satisfied within the search space, the pruning will be interrupted
and an errror will be raised.</p>
</div>
</section>
</section>
<section id="fine-tuning">
<span id="pruning-fine-tuning"></span><h2>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Link to this heading"></a></h2>
<p>The final step of architecture search is to fine-tune the pruned model on your dataset. This way
you can ensure to obtain the best possible performance for your pruned model.</p>
<section id="id1">
<h3>Prerequisites<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>To perform fine-tuning you need a pruned subnet as explained in the previous section.</p></li>
<li><p>You can reuse your existing training pipeline. We recommend running fine-tuning with your
original training schedule:</p>
<ul class="simple">
<li><p>1x training epochs (or 1x downstream task fine-tuning),</p></li>
<li><p>same or smaller (0.5x-1x) learning rate.</p></li>
</ul>
</li>
</ol>
</section>
<section id="load-the-pruned-model">
<h3>Load the pruned model<a class="headerlink" href="#load-the-pruned-model" title="Link to this heading"></a></h3>
<p>You can simply restore your pruned model (weights and architecture) using
<a class="reference internal" href="../reference/generated/modelopt.torch.opt.conversion.html#modelopt.torch.opt.conversion.restore" title="modelopt.torch.opt.conversion.restore"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mto.restore()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">modelopt.torch.opt</span> <span class="k">as</span> <span class="nn">mto</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="c1"># Build original model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">()</span>

<span class="c1"># Restore the pruned architecture and weights</span>
<span class="n">pruned_model</span> <span class="o">=</span> <span class="n">mto</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;modelopt_pruned_model.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="run-fine-tuning">
<h3>Run fine-tuning<a class="headerlink" href="#run-fine-tuning" title="Link to this heading"></a></h3>
<p>Now, please go ahead and fine-tune the pruned subnet using your standard training pipeline with
the pre-configured hyperparameters. A usually good fine-tuning schedule is
to repeat the pre-training schedule with 0.5x-1x initial learning rate.</p>
<p>Do not forget to save the model using <a class="reference internal" href="../reference/generated/modelopt.torch.opt.conversion.html#modelopt.torch.opt.conversion.save" title="modelopt.torch.opt.conversion.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">mto.save()</span></code></a>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">)</span>

<span class="n">mto</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">pruned_model</span><span class="p">,</span> <span class="s2">&quot;modelopt_pruned_finetuned_model.pth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="deploy">
<h3>Deploy<a class="headerlink" href="#deploy" title="Link to this heading"></a></h3>
<p>The pruned and finetuned model is now ready for downstream tasks like deployment. The model you
have in hand now should be the best neural network meeting your deployment-aware search constraint.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">modelopt.torch.opt</span> <span class="k">as</span> <span class="nn">mto</span>
<span class="kn">from</span> <span class="nn">torchvision.models</span> <span class="kn">import</span> <span class="n">resnet50</span>

<span class="c1"># Build original model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">resnet50</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">mto</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">&quot;modelopt_pruned_finetuned_model.pth&quot;</span><span class="p">)</span>

<span class="c1"># Continue with downstream tasks like deployment (e.g. TensorRT or TensorRT-LLM)</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
</section>
<section id="pruning-concepts">
<span id="id2"></span><h2>Pruning Concepts<a class="headerlink" href="#pruning-concepts" title="Link to this heading"></a></h2>
<p>Pruning is the process of removing redundant components from a neural network for a given task.
Conceptually, pruning is similar to NAS, but has less computational overhead compared to NAS at the
cost of potentially finding a less optimal architecture compared to NAS. Most APIs are based on the
corresponding NAS APIs but are adapted to reflect the simpler workflow.</p>
<p>Specifically, for pruning we do not specifically train the search space and all its subnets.
Instead, a pre-trained checkpoint is used to approximate the search space. Therefore, we can skip
the (potentially expensive) search space training step and directly
<a class="reference internal" href="3_nas.html#search-space-search-selection"><span class="std std-ref">search</span></a> for a subnet architecture before fine-tuning the
resulting subnet.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you want to learn more about the concept behind NAS and pruning, take a look at
<a class="reference internal" href="3_nas.html#nas-concepts"><span class="std std-ref">NAS Concepts</span></a> including a more detailed comparison between NAS and pruning.</p>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="_onnx_quantization.html" class="btn btn-neutral float-left" title="ONNX Quantization (Beta)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="3_nas.html" class="btn btn-neutral float-right" title="NAS" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023-2024, NVIDIA Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>